# AI Detector Testing Suite

Automated testing framework for validating Graspit's humanization effectiveness against industry-standard AI detectors (ZeroGPT, GPTZero).

## ðŸš€ Quick Start - GitHub Actions (RECOMMENDED)

**Zero local setup required! Run tests directly on GitHub.**

### Option 1: Manual Trigger
1. Go to your repo: `https://github.com/YOUR_USERNAME/graspit/actions`
2. Click "AI Detector Testing" workflow
3. Click "Run workflow"
4. Select sample: `heavy`, `medium`, or `all`
5. Click "Run workflow" button
6. Wait ~5 minutes for results
7. Download artifacts (screenshots + JSON results)

### Option 2: Scheduled (Weekly)
Tests run automatically every Sunday at 00:00 UTC.

### Option 3: Claude.ai Integration
```
Hey Claude, run AI detector tests on GitHub Actions for the graspit repo
```
Claude.ai can trigger the workflow and analyze results without touching your local machine!

---

## ðŸ“¦ What Gets Tested

### Heavy AI Sample (Academic Style)
- **Expected Original Score**: 80-95% AI
- **Target Humanized Score**: <25% AI
- **Key Indicators**: Em-dashes, formal vocab, passive voice, AI clichÃ©s

### Medium AI Sample (Blog Style)
- **Expected Original Score**: 50-70% AI
- **Target Humanized Score**: <20% AI
- **Key Indicators**: AI clichÃ©s, formal transitions, predictable structure

---

## ðŸ› ï¸ Local Testing (Optional)

### Prerequisites
```bash
# Python 3.11+
python3 --version

# Install dependencies
pip install -r testing/requirements.txt

# Chrome browser (auto-installed in GitHub Actions)
```

### Run Tests
```bash
# Test heavy AI sample
python testing/test-ai-detectors.py --sample heavy

# Test medium AI sample
python testing/test-ai-detectors.py --sample medium

# Test all samples
python testing/test-ai-detectors.py --all

# Run with visible browser (no headless)
python testing/test-ai-detectors.py --sample heavy --no-headless
```

---

## ðŸ“Š Results Format

### JSON Output (`testing/results/*.json`)
```json
{
  "sample": "heavy",
  "sample_name": "Heavy AI (Academic)",
  "timestamp": "2025-11-17T12:00:00",
  "original_text": "...",
  "humanized_text": "...",
  "zerogpt": {
    "original": {
      "score": "92%",
      "screenshot": "path/to/screenshot.png",
      "success": true
    },
    "humanized": {
      "score": "18%",
      "screenshot": "path/to/screenshot.png",
      "success": true
    }
  },
  "gptzero": {
    "original": {
      "result": "HIGH AI probability",
      "screenshot": "path/to/screenshot.png",
      "success": true
    },
    "humanized": {
      "result": "LOW AI probability",
      "screenshot": "path/to/screenshot.png",
      "success": true
    }
  }
}
```

### Screenshots
All detection results are screenshotted and saved to `testing/results/screenshots/`

---

## ðŸŽ¯ Success Criteria

**ZeroGPT**:
- âœ… Original: 80-95% AI detected
- âœ… Humanized: <25% AI detected
- âœ… Improvement: 60+ point reduction

**GPTZero**:
- âœ… Original: HIGH AI probability
- âœ… Humanized: LOW/MIXED probability
- âœ… Clear downgrade in AI confidence

---

## ðŸ¤– GitHub Actions Workflow

### Features:
- âœ… Parallel test execution (heavy + medium simultaneously)
- âœ… Headless Chrome (no GUI needed)
- âœ… Auto-upload results as artifacts
- âœ… 30-day artifact retention
- âœ… Weekly scheduled runs
- âœ… Manual trigger via web UI
- âœ… Claude.ai compatible

### Workflow File
`.github/workflows/test-ai-detectors.yml`

---

## ðŸ“ File Structure

```
testing/
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ test-ai-detectors.py          # Main automation script
â”œâ”€â”€ ai-detector-test.md           # Manual testing protocol
â””â”€â”€ results/                       # Test outputs (gitignored)
    â”œâ”€â”€ heavy_*.json
    â”œâ”€â”€ medium_*.json
    â””â”€â”€ screenshots/
        â”œâ”€â”€ zerogpt_*.png
        â””â”€â”€ gptzero_*.png
```

---

## ðŸ”„ Continuous Testing Strategy

### Weekly Regression Tests
- Automatic runs every Sunday
- Validates humanization consistency
- Tracks detector algorithm changes
- Maintains quality benchmarks

### On-Demand Testing
- Before major releases
- After humanization rule updates
- When testing new AI samples
- For marketing/proof demonstrations

### Result History
- Artifacts stored for 30 days in GitHub
- JSON results for trend analysis
- Screenshots for visual proof
- Exportable for documentation

---

## ðŸ› Troubleshooting

### Test Fails with Timeout
- **Cause**: Detector website slow/down
- **Fix**: Re-run workflow or increase wait times in script

### Unable to Extract Score
- **Cause**: Detector UI changed
- **Fix**: Update CSS selectors in `test-ai-detectors.py`

### Screenshot is Blank
- **Cause**: Page not fully loaded before screenshot
- **Fix**: Increase `time.sleep()` before screenshot

### GitHub Actions Fails
- **Cause**: Missing dependencies
- **Fix**: Check `requirements.txt` versions

---

## ðŸ“ Adding New Test Samples

Edit `test-ai-detectors.py`:

```python
SAMPLES = {
    "your_sample_name": {
        "name": "Descriptive Name",
        "text": """Your AI-generated text here...""",
        "expected_score": "XX-XX%"
    }
}
```

Then update workflow to include new sample.

---

## ðŸŽ“ Example: Claude.ai Workflow

```
User: "Test Graspit against AI detectors"

Claude.ai:
1. Triggers GitHub Actions workflow
2. Waits for completion (~5 mins)
3. Downloads artifacts
4. Analyzes JSON results
5. Compares scores
6. Generates summary report
7. Provides improvement recommendations

All without touching your local machine or API limits!
```

---

## ðŸ“ˆ Metrics Tracked

- **Original AI Score**: Baseline detection rate
- **Humanized AI Score**: Post-Graspit detection rate
- **Improvement Delta**: Point reduction
- **Success Rate**: % of tests passing criteria
- **Consistency**: Score variance across runs
- **Detector Reliability**: Cross-platform comparison

---

## ðŸš€ Future Enhancements

- [ ] Add Originality.ai testing
- [ ] Add Writer.com AI detector
- [ ] Implement quiz auto-solving for full end-to-end
- [ ] Add trend analysis dashboard
- [ ] Export results to Google Sheets
- [ ] Slack/Discord notifications
- [ ] A/B testing different humanization strategies

---

**Built with ðŸ’™ by Dash & ZION**
*Automated testing for human-first AI content*
